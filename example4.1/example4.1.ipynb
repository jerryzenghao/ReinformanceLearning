{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4.1\n",
    "\n",
    "Let \\{0,1,2,3\\} denote  the actions \\{up, right, down, left\\} respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class gridworld:\n",
    "    def __init__(self):\n",
    "        self.terminal_state = [0,15]\n",
    "        self.action = [0,1,2,3]\n",
    "        self.value = np.zeros(16)\n",
    "        self.reward = -1 \n",
    "    \n",
    "    def next_state(self, s, a):\n",
    "        if s in self.terminal_state:\n",
    "           return s\n",
    "        if a == 0:\n",
    "            s = s - 4 if s >= 4 else s  \n",
    "        elif a == 1:\n",
    "            s = s + 1 if (s+1)%4 != 0 else s\n",
    "        elif a == 2:\n",
    "            s = s + 4 if (s+4) < 16 else s\n",
    "        elif a == 3:\n",
    "            s = s - 1 if s % 4 !=0 else s\n",
    "        return s\n",
    "     \n",
    "    def policy_evaluation(self):\n",
    "         k=0\n",
    "         while True:\n",
    "             delta = 0\n",
    "             v_new = np.zeros(16)\n",
    "             for s in range(16):\n",
    "                 v = 0\n",
    "                 if s in self.terminal_state:\n",
    "                     v_new[s] = 0\n",
    "                 else:\n",
    "                     v = self.value[s]\n",
    "                     temp = 0\n",
    "                     for a in range(4):\n",
    "                        v_new[s] += 0.25*(-1 + self.value[self.next_state(s,a)])\n",
    "                 delta = max(delta, abs(v-v_new[s]))\n",
    "             self.value = v_new\n",
    "             \n",
    "             # greedy policy\n",
    "             policy = np.zeros((16,4))\n",
    "             for s in range(1,15):\n",
    "                 vmax = -30\n",
    "                 nmax = 0\n",
    "                 for a in range(4):\n",
    "                     v = self.value[self.next_state(s,a)]\n",
    "                     if v > vmax:\n",
    "                         vmax = v\n",
    "                         nmax = 1\n",
    "                     elif vmax - v < 0.05:\n",
    "                         nmax += 1\n",
    "                 for a in range(4):\n",
    "                     v = self.value[self.next_state(s,a)]\n",
    "                     if v == vmax:\n",
    "                         policy[s,a] = 1/nmax\n",
    "        \n",
    "             k += 1\n",
    "             if k == 1 or k==2 or k==3 or k==10 or k==131:\n",
    "                print('The state value for %ith iteration:\\n' %k, v_new.reshape((4,4)))\n",
    "                print('The greedy policy for %ith iteration:\\n' %k, policy)\n",
    "             if delta < 0.001:\n",
    "                 return\n",
    "\n",
    "        \n",
    "              \n",
    "\n",
    "\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The state value for 1th iteration:\n [[ 0. -1. -1. -1.]\n [-1. -1. -1. -1.]\n [-1. -1. -1. -1.]\n [-1. -1. -1.  0.]]\nThe greedy policy for 1th iteration:\n [[0.   0.   0.   0.  ]\n [0.   0.   0.   1.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   1.   0.   0.  ]\n [0.   0.   0.   0.  ]]\nThe state value for 2th iteration:\n [[ 0.   -1.75 -2.   -2.  ]\n [-1.75 -2.   -2.   -2.  ]\n [-2.   -2.   -2.   -1.75]\n [-2.   -2.   -1.75  0.  ]]\nThe greedy policy for 2th iteration:\n [[0.   0.   0.   0.  ]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [0.25 0.25 0.25 0.25]\n [1.   0.   0.   0.  ]\n [0.5  0.   0.   0.5 ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.5  0.5  0.  ]\n [0.   0.   1.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   1.   0.   0.  ]\n [0.   1.   0.   0.  ]\n [0.   0.   0.   0.  ]]\nThe state value for 3th iteration:\n [[ 0.     -2.4375 -2.9375 -3.    ]\n [-2.4375 -2.875  -3.     -2.9375]\n [-2.9375 -3.     -2.875  -2.4375]\n [-3.     -2.9375 -2.4375  0.    ]]\nThe greedy policy for 3th iteration:\n [[0.  0.  0.  0. ]\n [0.  0.  0.  1. ]\n [0.  0.  0.  1. ]\n [0.  0.  0.5 0.5]\n [1.  0.  0.  0. ]\n [0.5 0.  0.  0.5]\n [0.  0.  0.5 0.5]\n [0.  0.  1.  0. ]\n [1.  0.  0.  0. ]\n [0.5 0.5 0.  0. ]\n [0.  0.5 0.5 0. ]\n [0.  0.  1.  0. ]\n [0.5 0.5 0.  0. ]\n [0.  1.  0.  0. ]\n [0.  1.  0.  0. ]\n [0.  0.  0.  0. ]]\nThe state value for 10th iteration:\n [[ 0.         -6.13796997 -8.35235596 -8.96731567]\n [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n [-8.96731567 -8.35235596 -6.13796997  0.        ]]\nThe greedy policy for 10th iteration:\n [[0.  0.  0.  0. ]\n [0.  0.  0.  1. ]\n [0.  0.  0.  1. ]\n [0.  0.  0.5 0.5]\n [1.  0.  0.  0. ]\n [0.5 0.  0.  0.5]\n [0.  0.  0.5 0.5]\n [0.  0.  1.  0. ]\n [1.  0.  0.  0. ]\n [0.5 0.5 0.  0. ]\n [0.  0.5 0.5 0. ]\n [0.  0.  1.  0. ]\n [0.5 0.5 0.  0. ]\n [0.  1.  0.  0. ]\n [0.  1.  0.  0. ]\n [0.  0.  0.  0. ]]\nThe state value for 131th iteration:\n [[  0.         -13.98945772 -19.98437823 -21.98251832]\n [-13.98945772 -17.98623815 -19.98448273 -19.98437823]\n [-19.98437823 -19.98448273 -17.98623815 -13.98945772]\n [-21.98251832 -19.98437823 -13.98945772   0.        ]]\nThe greedy policy for 131th iteration:\n [[0.  0.  0.  0. ]\n [0.  0.  0.  1. ]\n [0.  0.  0.  1. ]\n [0.  0.  0.5 0.5]\n [1.  0.  0.  0. ]\n [0.5 0.  0.  0. ]\n [0.  0.  0.5 0.5]\n [0.  0.  1.  0. ]\n [1.  0.  0.  0. ]\n [0.5 0.5 0.  0. ]\n [0.  0.5 0.5 0. ]\n [0.  0.  1.  0. ]\n [0.5 0.5 0.  0. ]\n [0.  1.  0.  0. ]\n [0.  1.  0.  0. ]\n [0.  0.  0.  0. ]]\n"
    }
   ],
   "source": [
    "a.policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}